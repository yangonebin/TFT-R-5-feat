import os
import yfinance as yf
import pandas as pd
import numpy as np
import warnings
import mlflow
import mlflow.keras
import tensorflow as tf
from tensorflow.keras import layers, Model, Input
from sklearn.preprocessing import StandardScaler
import time

# ìµœì í™” ë° ê²½ê³  ë¬´ì‹œ
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
warnings.filterwarnings('ignore')

# ğŸ† ê²€ì¦ ìµœê³  ìˆ˜ìµë¥  ì‹œë“œ (15) ì ìš©
SEED = 15
os.environ['PYTHONHASHSEED'] = str(SEED)
tf.random.set_seed(SEED)
np.random.seed(SEED)

# --- TFTë¥¼ ìœ„í•œ í•µì‹¬ ì»¤ìŠ¤í…€ ë¸”ë¡ ---
def gated_residual_network(x, units, dropout_rate=0.1):
    h = layers.Dense(units, activation='elu')(x)
    h = layers.Dense(units)(h)
    h = layers.Dropout(dropout_rate)(h)
    gate = layers.Dense(units, activation='sigmoid')(x)
    x = layers.Add()([x, layers.Multiply()([gate, h])])
    return layers.LayerNormalization()(x)

# --- [VSN í•¨ìˆ˜] ---
def variable_selection_network(x, units, num_features):
    feature_embeddings = []
    for i in range(num_features):
        feat = layers.Lambda(lambda x, i=i: x[:, :, i:i+1])(x)
        feature_embeddings.append(layers.Dense(units)(feat))
        
    combined = layers.Concatenate()(feature_embeddings)
    weights = layers.Dense(num_features, activation='softmax')(combined)
    
    stacked_features = layers.Lambda(lambda x: tf.stack(x, axis=2))(feature_embeddings)
    expanded_weights = layers.Reshape((-1, num_features, 1))(weights)
    weighted_features = layers.Multiply()([stacked_features, expanded_weights])
    
    return layers.Lambda(lambda x: tf.reduce_sum(x, axis=2))(weighted_features)

# --- ë°ì´í„° ë¹Œë” (Bitcoin ì „ìš©) ---
def build_tft_data():
    target_stock = "BTC-USD"
    print(f"ğŸ“¥ {target_stock} ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
    
    df = yf.download(target_stock, period="max", auto_adjust=True, progress=False)
    if isinstance(df.columns, pd.MultiIndex): df.columns = df.columns.get_level_values(0)
    
    df = df[['Open', 'High', 'Low', 'Close', 'Volume']].ffill().dropna()
    feature_cols = []
    
    # ë‚˜ì¤‘ì— ìµœì‹  ë°ì´í„° ì¶”ë¡ ì„ ìœ„í•´ ì›ë³¸ ë°ì´í„° ë³´ì¡´
    global df_raw
    df_raw = df.copy()

    for col in df.columns:
        new_name = f'Log_Ret_{col}'
        df[new_name] = np.log((df[col] + 1e-9) / (df[col].shift(1) + 1e-9))
        feature_cols.append(new_name)
    
    df['target_up'] = np.where(df['Close'].shift(-1) > df['Close'], 1, 0)
    df['actual_ret'] = (df['Close'].shift(-1) / df['Close']) - 1
    df = df.dropna()
    
    scaler = StandardScaler()
    # feature_cols ì „ì²´ì— ëŒ€í•´ fit_transform
    df[feature_cols] = scaler.fit_transform(df[feature_cols])
    
    # ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ë‚˜ì¤‘ì— ì“°ê¸° ìœ„í•´ ì „ì—­ ë³€ìˆ˜ë‚˜ ë¦¬í„´ê°’ìœ¼ë¡œ ì €ì¥
    global trained_scaler
    trained_scaler = scaler
    
    return df, feature_cols

def create_sequences(df, features, window=20):
    X, y, r = [], [], []
    data, target, ret = df[features].values, df['target_up'].values, df['actual_ret'].values
    for i in range(len(df) - window):
        X.append(data[i:i+window])
        y.append(target[i+window-1])
        r.append(ret[i+window-1])
    return np.array(X), np.array(y), np.array(r)

def build_beast_tft(window_size, num_features, units=64):
    inputs = Input(shape=(window_size, num_features))
    vsn = variable_selection_network(inputs, units, num_features)
    lstm = layers.LSTM(units, return_sequences=True)(vsn)
    lstm = layers.LayerNormalization()(lstm)
    attn = layers.MultiHeadAttention(num_heads=4, key_dim=units)(lstm, lstm)
    attn = layers.Add()([lstm, attn])
    attn = layers.LayerNormalization()(attn)
    grn = gated_residual_network(attn[:, -1, :], units)
    outputs = layers.Dense(1)(grn) 
    model = Model(inputs, outputs)
    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='mse')
    return model

if __name__ == "__main__":
    # 1. ë°ì´í„° ì¤€ë¹„
    df, features = build_tft_data()
    X, y, r = create_sequences(df, features)
    
    # 2. í•œë¹ˆë‹˜ì´ ì›í•˜ì‹  ê·¸ëŒ€ë¡œ Split ë° íŒŒë¼ë¯¸í„° ìœ ì§€
    split = int(len(X) * 0.8)
    X_train, X_test = X[:split], X[split:]
    y_train, y_test = r[:split], r[split:] 
    
    print(f"\nğŸš€ [Beast V3] ë¹„íŠ¸ì½”ì¸ í•™ìŠµ ì‹œì‘ (Epochs: 150, ES: 15)...")
    
    # 3. ëª¨ë¸ ë¹Œë“œ ë° í•™ìŠµ (Validation Data í¬í•¨ ìœ ì§€)
    model = build_beast_tft(X.shape[1], X.shape[2])
    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)
    
    model.fit(X_train, y_train, validation_data=(X_test, y_test), 
              epochs=150, batch_size=128, verbose=1, callbacks=[es])
    
    print("âœ… í•™ìŠµ ì™„ë£Œ. ë‚´ì¼ ì˜ˆì¸¡ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    # --- [ì‹¤ì „ ì˜ˆì¸¡: ë‚´ì¼ ë¹„íŠ¸ì½”ì¸ ì‚´ê¹Œ ë§ê¹Œ?] ---
    
    # 1. ê°€ì¥ ìµœì‹  20ì¼ì¹˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (df_raw ì‚¬ìš©)
    last_20_days = df_raw.tail(21).copy() # shift ê³„ì‚° ìœ„í•´ 21ê°œ í•„ìš”
    
    recent_features = pd.DataFrame()
    for col in ['Open', 'High', 'Low', 'Close', 'Volume']:
        new_name = f'Log_Ret_{col}'
        recent_features[new_name] = np.log((last_20_days[col] + 1e-9) / (last_20_days[col].shift(1) + 1e-9))
    
    # NaN ì œê±° í›„ ë§ˆì§€ë§‰ 20ê°œë§Œ ë‚¨ê¹€
    recent_features = recent_features.dropna().tail(20)
    
    # 2. í•™ìŠµ ë•Œ ì“´ ìŠ¤ì¼€ì¼ëŸ¬ë¡œ ë³€í™˜
    input_data = trained_scaler.transform(recent_features)
    input_seq = np.expand_dims(input_data, axis=0) # (1, 20, 5)
    
    # 3. ì˜ˆì¸¡
    prediction = float(model.predict(input_seq, verbose=0)[0][0])
    pred_percent = prediction * 100
    
    # 4. ê²°ê³¼ ì¶œë ¥ (ì‹¬í”Œí•˜ê²Œ)
    current_price = df_raw['Close'].iloc[-1]
    
    print("\n" + "="*40)
    print(f"ğŸ’° í˜„ì¬ ë¹„íŠ¸ì½”ì¸ ê°€ê²©: ${current_price:,.2f}")
    print(f"ğŸ¯ ë‚´ì¼ ì˜ˆìƒ ìˆ˜ìµë¥  : {pred_percent:+.4f}%")
    print("-" * 40)
    
    if prediction > 0:
        print("ğŸ”¥ [ê²°ë¡ ] : ì‚¬ë¼ (BUY)")
    else:
        print("â„ï¸ [ê²°ë¡ ] : íŒ”ì•„ë¼ (SELL/HOLD)")
    print("="*40)